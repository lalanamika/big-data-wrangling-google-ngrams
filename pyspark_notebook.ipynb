{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bdd4e55e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Big Data Wrangling With Google Books Ngrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6f0cf02",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "- [Big Data Wrangling With Google Books Ngrams](#big-data-wrangling-with-google-books-ngrams)\n",
    "  - [Introduction  ](#introduction--)\n",
    "  - [Setup Spark and import any packages needed on the head node.](#setup-spark-and-import-any-packages-needed-on-the-head-node.)\n",
    "  - [4. Using pyspark, read the data you copied into HDFS in Step 3. Once you have created a pyspark DataFrame, complete the subparts of the question:](#4.-using-pyspark,-read-the-data-you-copied-into-hdfs-in-step-3.-once-you-have-created-a-pyspark-dataframe,-complete-the-subparts-of-the-question:)\n",
    "  - [4.a. Describe the dataset (examples include size, shape, schema) in pyspark](#4.a.-describe-the-dataset-(examples-include-size,-shape,-schema)-in-pyspark)\n",
    "  - [4.b Create a new DataFrame from a query using Spark SQL, filtering to include only the rows where the token is \"data\" and describe the new dataset](#4.b-create-a-new-dataframe-from-a-query-using-spark-sql,-filtering-to-include-only-the-rows-where-the-token-is-\"data\"-and-describe-the-new-dataset)\n",
    "    - [Method 1 - Using filter](#method-1---using-filter)\n",
    "    - [Method 2 - Using Spark SQL](#method-2---using-spark-sql)\n",
    "  - [4.c. Write the filtered data back to a directory in the HDFS from Spark using df.write.csv(). Be sure to pass the header=True parameter and examine the contents of what you've written.](#4.c.-write-the-filtered-data-back-to-a-directory-in-the-hdfs-from-spark-using-df.write.csv().-be-sure-to-pass-the-header=true-parameter-and-examine-the-contents-of-what-you've-written.)\n",
    "  - [5. Collect the contents of the directory into a single file on the local drive of the head node using getmerge and move this file to your local machine / laptop.](#5.-collect-the-contents-of-the-directory-into-a-single-file-on-the-local-drive-of-the-head-node-using-getmerge-and-move-this-file-to-your-local-machine-/-laptop.)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6efc4c",
   "metadata": {},
   "source": [
    "## Introduction  \n",
    "\n",
    "This notebook was run on the remote EMR Spark AWS cluster and addresses question 4 and 5 of the deliverable.\n",
    "\n",
    "Before starting this notebook, we did the steps for questions 1 to 3.\n",
    "\n",
    "1. Spin up a new EMR cluster on AWS for using Spark and EMR notebooks\n",
    "2. Connect to the head node of the cluster using SSH.\n",
    "3. Copy the data folder from the S3 bucket directly into a directory on the Hadoop File System (HDFS) named /user/hadoop/eng_1M_1gram."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304d61be",
   "metadata": {},
   "source": [
    "## Setup Spark and import any packages needed on the head node."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0ae4df2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Spark application\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1719338465640_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-90-118.ec2.internal:20888/proxy/application_1719338465640_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-92-4.ec2.internal:8042/node/containerlogs/container_1719338465640_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SparkSession available as 'spark'.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1719338465640_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-90-118.ec2.internal:20888/proxy/application_1719338465640_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-92-4.ec2.internal:8042/node/containerlogs/container_1719338465640_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%configure -f\n",
    "{\n",
    "    \"conf\": {\n",
    "        \"spark.pyspark.python\": \"python3\",\n",
    "        \"spark.pyspark.virtualenv.enabled\": \"true\",\n",
    "        \"spark.pyspark.virtualenv.type\":\"native\",\n",
    "        \"spark.pyspark.virtualenv.bin.path\":\"/usr/bin/virtualenv\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f9f29721",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pyspark.sql.session.SparkSession object at 0x7f7cb0654510>"
     ]
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cc0acf85",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Package                    Version\n",
      "-------------------------- ----------\n",
      "aws-cfn-bootstrap          2.0\n",
      "beautifulsoup4             4.9.3\n",
      "boto                       2.49.0\n",
      "click                      8.1.3\n",
      "docutils                   0.14\n",
      "jmespath                   1.0.1\n",
      "joblib                     1.2.0\n",
      "lockfile                   0.11.0\n",
      "lxml                       4.9.2\n",
      "mysqlclient                1.4.2\n",
      "nltk                       3.8\n",
      "nose                       1.3.4\n",
      "numpy                      1.20.0\n",
      "pip                        20.2.2\n",
      "py-dateutil                2.2\n",
      "pystache                   0.5.4\n",
      "python-daemon              2.2.3\n",
      "python37-sagemaker-pyspark 1.4.2\n",
      "pytz                       2022.7\n",
      "PyYAML                     5.4.1\n",
      "regex                      2021.11.10\n",
      "setuptools                 28.8.0\n",
      "simplejson                 3.2.0\n",
      "six                        1.13.0\n",
      "tqdm                       4.64.1\n",
      "wheel                      0.29.0\n",
      "windmill                   1.6\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "sc.list_packages()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "917773b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Current session configs: <tt>{'conf': {'spark.pyspark.python': 'python3', 'spark.pyspark.virtualenv.enabled': 'true', 'spark.pyspark.virtualenv.type': 'native', 'spark.pyspark.virtualenv.bin.path': '/usr/bin/virtualenv'}, 'proxyUser': 'jovyan', 'kind': 'pyspark'}</tt><br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table>\n",
       "<tr><th>ID</th><th>YARN Application ID</th><th>Kind</th><th>State</th><th>Spark UI</th><th>Driver log</th><th>User</th><th>Current session?</th></tr><tr><td>1</td><td>application_1719338465640_0003</td><td>pyspark</td><td>idle</td><td><a target=\"_blank\" href=\"http://ip-172-31-90-118.ec2.internal:20888/proxy/application_1719338465640_0003/\">Link</a></td><td><a target=\"_blank\" href=\"http://ip-172-31-92-4.ec2.internal:8042/node/containerlogs/container_1719338465640_0003_01_000001/livy\">Link</a></td><td>None</td><td>✔</td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e1699af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandas==1.0.5\n",
      "  Downloading pandas-1.0.5-cp37-cp37m-manylinux1_x86_64.whl (10.1 MB)\n",
      "Collecting python-dateutil>=2.6.1\n",
      "  Downloading python_dateutil-2.9.0.post0-py2.py3-none-any.whl (229 kB)\n",
      "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/site-packages (from pandas==1.0.5) (2022.7)\n",
      "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib64/python3.7/site-packages (from pandas==1.0.5) (1.20.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.6.1->pandas==1.0.5) (1.13.0)\n",
      "Installing collected packages: python-dateutil, pandas\n",
      "Successfully installed pandas-1.0.5 python-dateutil-2.9.0.post0\n",
      "\n",
      "Collecting matplotlib==3.1.1\n",
      "  Downloading matplotlib-3.1.1-cp37-cp37m-manylinux1_x86_64.whl (13.1 MB)\n",
      "Collecting cycler>=0.10\n",
      "  Downloading cycler-0.11.0-py3-none-any.whl (6.4 kB)\n",
      "Collecting pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1\n",
      "  Downloading pyparsing-3.1.2-py3-none-any.whl (103 kB)\n",
      "Requirement already satisfied: numpy>=1.11 in /usr/local/lib64/python3.7/site-packages (from matplotlib==3.1.1) (1.20.0)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in ./tmp/1719341098710-0/lib/python3.7/site-packages (from matplotlib==3.1.1) (2.9.0.post0)\n",
      "Collecting kiwisolver>=1.0.1\n",
      "  Downloading kiwisolver-1.4.5-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/site-packages (from python-dateutil>=2.1->matplotlib==3.1.1) (1.13.0)\n",
      "Collecting typing-extensions; python_version < \"3.8\"\n",
      "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
      "Installing collected packages: cycler, pyparsing, typing-extensions, kiwisolver, matplotlib\n",
      "Successfully installed cycler-0.11.0 kiwisolver-1.4.5 matplotlib-3.1.1 pyparsing-3.1.2 typing-extensions-4.7.1\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag.\n",
      "\n",
      "WARNING: The directory '/home/.cache/pip' or its parent directory is not owned or is not writable by the current user. The cache has been disabled. Check the permissions and owner of that directory. If executing pip with sudo, you may want sudo's -H flag."
     ]
    }
   ],
   "source": [
    "# install data science & plotting packages\n",
    "# this is the spark version of pip\n",
    "\n",
    "sc.install_pypi_package(\"pandas==1.0.5\")\n",
    "sc.install_pypi_package(\"matplotlib==3.1.1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "168be296",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "# only on head node"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab652c7c",
   "metadata": {},
   "source": [
    "Let us answer question 4 now."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80e35896",
   "metadata": {},
   "source": [
    "## 4. Using pyspark, read the data you copied into HDFS in Step 3. Once you have created a pyspark DataFrame, complete the subparts of the question:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cc9fdf36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df = spark.read.csv('/user/hadoop/eng_1M_1gram',\n",
    "                    header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ef7b965",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- frequency: string (nullable = true)\n",
      " |-- pages: string (nullable = true)\n",
      " |-- books: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3288326",
   "metadata": {},
   "source": [
    "## 4.a. Describe the dataset (examples include size, shape, schema) in pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a6a25cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'year', 'frequency', 'pages', 'books']"
     ]
    }
   ],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35c559ff",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(token='inGermany', year='1927', frequency='2', pages='2', books='2'), Row(token='inGermany', year='1929', frequency='1', pages='1', books='1'), Row(token='inGermany', year='1930', frequency='1', pages='1', books='1'), Row(token='inGermany', year='1933', frequency='1', pages='1', books='1'), Row(token='inGermany', year='1934', frequency='1', pages='1', books='1')]"
     ]
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "df99dd0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----+---------+-----+-----+\n",
      "|    token|year|frequency|pages|books|\n",
      "+---------+----+---------+-----+-----+\n",
      "|inGermany|1927|        2|    2|    2|\n",
      "|inGermany|1929|        1|    1|    1|\n",
      "|inGermany|1930|        1|    1|    1|\n",
      "|inGermany|1933|        1|    1|    1|\n",
      "|inGermany|1934|        1|    1|    1|\n",
      "|inGermany|1935|        1|    1|    1|\n",
      "|inGermany|1938|        5|    5|    5|\n",
      "|inGermany|1939|        1|    1|    1|\n",
      "|inGermany|1940|        1|    1|    1|\n",
      "|inGermany|1942|        2|    2|    2|\n",
      "+---------+----+---------+-----+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2e4e3083",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "261823225"
     ]
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a7f2e6d",
   "metadata": {},
   "source": [
    "The dataset has 261823225 rows (~261 million) and 5 columns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3cbcaa2",
   "metadata": {},
   "source": [
    "## 4.b Create a new DataFrame from a query using Spark SQL, filtering to include only the rows where the token is \"data\" and describe the new dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8c0b17a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3223095"
     ]
    }
   ],
   "source": [
    "df.select('token').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d29dd228",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "+-----+----+---------+-----+-----+"
     ]
    }
   ],
   "source": [
    "df.filter(df['token'] == \"inData\").show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e0b7d59",
   "metadata": {},
   "source": [
    "### Method 1 - Using filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "639b66bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "| data|1584|       16|   14|    1|\n",
      "| data|1614|        3|    2|    1|\n",
      "+-----+----+---------+-----+-----+\n",
      "only showing top 2 rows"
     ]
    }
   ],
   "source": [
    "df.filter(df['token'] == \"data\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "63ba601d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_data = df.filter(df['token'] == \"data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e72832ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- frequency: string (nullable = true)\n",
      " |-- pages: string (nullable = true)\n",
      " |-- books: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_data.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d26f098b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'year', 'frequency', 'pages', 'books']"
     ]
    }
   ],
   "source": [
    "df_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a0f1676a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "| data|1584|       16|   14|    1|\n",
      "| data|1614|        3|    2|    1|\n",
      "| data|1627|        1|    1|    1|\n",
      "| data|1631|       22|   18|    1|\n",
      "| data|1637|        1|    1|    1|\n",
      "| data|1638|        2|    2|    1|\n",
      "| data|1640|        1|    1|    1|\n",
      "| data|1642|        1|    1|    1|\n",
      "| data|1644|        4|    4|    1|\n",
      "| data|1647|        1|    1|    1|\n",
      "+-----+----+---------+-----+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_data.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c8a4c2f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316"
     ]
    }
   ],
   "source": [
    "df_data.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439243af",
   "metadata": {},
   "source": [
    "### Method 2 - Using Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04a0cf7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df.createOrReplaceTempView(\"ngrams\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9b00aa34",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "| data|1584|       16|   14|    1|\n",
      "| data|1614|        3|    2|    1|\n",
      "| data|1627|        1|    1|    1|\n",
      "| data|1631|       22|   18|    1|\n",
      "| data|1637|        1|    1|    1|\n",
      "| data|1638|        2|    2|    1|\n",
      "| data|1640|        1|    1|    1|\n",
      "| data|1642|        1|    1|    1|\n",
      "| data|1644|        4|    4|    1|\n",
      "| data|1647|        1|    1|    1|\n",
      "+-----+----+---------+-----+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "spark.sql(\"SELECT * FROM ngrams WHERE token='data'\").show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "14f17e0e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sql = spark.sql(\"SELECT * FROM ngrams WHERE token='data'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "27028c07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316"
     ]
    }
   ],
   "source": [
    "df_sql.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dec7cce",
   "metadata": {},
   "source": [
    "The filtered dataset has 316 rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4b77661c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- token: string (nullable = true)\n",
      " |-- year: string (nullable = true)\n",
      " |-- frequency: string (nullable = true)\n",
      " |-- pages: string (nullable = true)\n",
      " |-- books: string (nullable = true)"
     ]
    }
   ],
   "source": [
    "df_sql.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2ae5ce1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['token', 'year', 'frequency', 'pages', 'books']"
     ]
    }
   ],
   "source": [
    "df_sql.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590d1ad6",
   "metadata": {},
   "source": [
    "## 4.c. Write the filtered data back to a directory in the HDFS from Spark using df.write.csv(). Be sure to pass the header=True parameter and examine the contents of what you've written."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "092528b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_sql.write.csv(\"ngram_data.csv\",header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "ba763021",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# double checking that the csv was written properly\n",
    "df_test = spark.read.csv(\"/user/livy/ngram_data.csv\", header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "bb0951b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+---------+-----+-----+\n",
      "|token|year|frequency|pages|books|\n",
      "+-----+----+---------+-----+-----+\n",
      "| data|1584|       16|   14|    1|\n",
      "| data|1614|        3|    2|    1|\n",
      "| data|1627|        1|    1|    1|\n",
      "| data|1631|       22|   18|    1|\n",
      "| data|1637|        1|    1|    1|\n",
      "| data|1638|        2|    2|    1|\n",
      "| data|1640|        1|    1|    1|\n",
      "| data|1642|        1|    1|    1|\n",
      "| data|1644|        4|    4|    1|\n",
      "| data|1647|        1|    1|    1|\n",
      "+-----+----+---------+-----+-----+\n",
      "only showing top 10 rows"
     ]
    }
   ],
   "source": [
    "df_test.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "13759771",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, bar_style='info', description='Progress:', layout=Layout(height='25px', width='50%'),…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "316"
     ]
    }
   ],
   "source": [
    "df_test.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be86e606",
   "metadata": {},
   "source": [
    "## 5. Collect the contents of the directory into a single file on the local drive of the head node using getmerge and move this file to your local machine / laptop."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1482184d",
   "metadata": {},
   "source": [
    "I did the following steps to do this:\n",
    "My file name was ngram_data.csv\n",
    "1. hadoop fs -ls /user/livy to verify that the file showed up there\n",
    "2. hadoop fs -getmerge /user/livy/ngram_data.csv ngramLocal.csv to merge the csv and save to the local directory on the cluster\n",
    "3. sudo cp ngramLocal.csv  /mnt/var/lib/jupyter/home/jovyan/ to move it to a location I can download it from\n",
    "4. Download ngramLocal.csv by going to https://localhost:9995 and selecting the file and hitting Download\n",
    "5. Verifying that the file had downloaded onto my laptop.\n",
    "6. Loaded it into a notebook to make sure it showed up properly."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pysparkkernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "python",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "pyspark",
   "pygments_lexer": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
